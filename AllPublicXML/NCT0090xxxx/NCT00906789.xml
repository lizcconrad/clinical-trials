<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on May 29, 2018</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT00906789</url>
  </required_header>
  <id_info>
    <org_study_id>Riverain SV 2.0.1 and OG5.0</org_study_id>
    <secondary_id>Softview 2.0.1</secondary_id>
    <secondary_id>OnGuard 5.0</secondary_id>
    <nct_id>NCT00906789</nct_id>
  </id_info>
  <brief_title>Testing of Computer Aided Detection Software for Riverain Medical Group</brief_title>
  <official_title>Testing of Computer Aided Detection Software for Riverain Medical Group</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>Georgetown University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>Riverain Technologies</agency>
      <agency_class>Industry</agency_class>
    </collaborator>
  </sponsors>
  <source>Georgetown University</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
  </oversight_info>
  <brief_summary>
    <textblock>
      This is a clinical trial using retrospective data of two different software devices developed
      by Riverain Medical Group: Softview and OnGuard 5.0. The two studies will be run
      concurrently. Riverain Medical Group's computer systems are designed to assist radiologists
      in their identification of lung cancer on chest radiographs. The current machine received FDA
      Pre-Market Approval. This is to test two new software approaches.
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      In 2000, data was presented to the FDA to demonstrate that a new system for computer analysis
      could assist radiologists in the detection of small lung cancers on chest radiographs.
      Radiologists using the system showed a statistically significant improvement in lung cancer
      detection rate when they used the system, compared to their interpretation of chest
      radiographs when they did not use the computer system. This study, along with other
      supporting data, resulted in the FDA giving Pre-Market Approval for the system.

      The system has undergone several improvements in software and hardware, and it is now
      intended to test two different software systems to determine whether radiologists using the
      systems can improve their detection of lung cancer on chest radiographs.

      One of these systems processes the chest radiograph to decrease the emphasis given to the
      shadow of the ribs and thereby enhances the ability of radiologists to detect disease in the
      lungs. The second system performs a series of evaluations on chest radiographs and, based on
      a complex system of analysis, points to locations on the chest radiograph that contain
      solitary pulmonary nodules having the characteristics of primary lung cancer or solitary
      metastases of cancer to the lungs.

      This will be a test of radiologists to determine the degree of improvement, if any, that
      results when they interpret chest radiographs that may or may not have cancer, first
      interpreted without the computer and, second, with the images output by the software.
    </textblock>
  </detailed_description>
  <overall_status>Completed</overall_status>
  <start_date>May 2009</start_date>
  <completion_date type="Actual">June 2010</completion_date>
  <primary_completion_date type="Actual">June 2010</primary_completion_date>
  <study_type>Observational</study_type>
  <has_expanded_access>No</has_expanded_access>
  <primary_outcome>
    <measure>Improvement in Cancer Detection as Measured by Localized Receiver Operating Characteristic) LROC Changes Under the LROC Curve.</measure>
    <time_frame>Three days of experiment over 3-5 months, varied by participant</time_frame>
    <description>Standard methods for LROC methodology and statistical analysis were used. We are testing two different types of software using different cases, but the same radiologists to control for radiologist differences. LROC is Localized Receiver Operating Characteristic. LROC measures the trade-offs between sensitivity and specificity as radiologists use different levels of suspicion of disease. This analysis is for the software that decreases the visibility of the ribs and clavicles while preserving (and potentially enhancing) the visibility of the lungs and lung diseases. In this case, the level of suspicion recorded was for the radiologist's concern that a finding did or did not represent cancer. Please note that the FDA approved indications for use is to detected nodules that may represent cancer, but in our study scoring for a true finding was based on whether or not the nodule did represent cancer.
A larger number, if statistically significant, indicates that that method is better.</description>
  </primary_outcome>
  <secondary_outcome>
    <measure>Sensitivity and Specificity Using SoftView Software</measure>
    <time_frame>Three days of experiment over 3-5 months, varied by participant</time_frame>
    <description>Sensitivity and specificity were calculated using the radiologists' responses of recommendations for follow-up with CT or biopsy. Truth was whether or not the nodule identified was found to be cancer. Sensitivity is the percentage of correct identification of a positive case (a case with cancer). Specificity is the percentage of negative cases (those without cancer) that were correctly identified as not having cancer. The mean values of 15 radiologists are used.</description>
  </secondary_outcome>
  <other_outcome>
    <measure>Difference in the Area Under the LROC Curve Comparing OnGuard 1.0 and OnGuard 5.1</measure>
    <time_frame>5 months</time_frame>
    <description>This reports the comparison of the detection of lung nodules that were proven to represent lung cancers. It compares the results of two versions of computer-aided detection software: OnGuard 1.0 from 2001 and OnGuard 5.1 from 2009. The results represent the responses of radiologists when they use one or the other types of software. To compare radiologists' results with the two types of software, the measurement analyzed was the difference in the areas under the localized receiver operating characteristic curve (LROC). The results from the 15 participating radiologists were averaged (mean value). The area under the LROC curve is a measure of the trade-offs between sensitivity and 1-specificity that occurs as the level of certainty of a positive finding changes. It is normally reported as a decimal without units. In this study dsign, a lower number indicates that the new method (OnGuard 5.1), if statistically significant, if better.</description>
  </other_outcome>
  <number_of_groups>1</number_of_groups>
  <enrollment type="Actual">15</enrollment>
  <condition>Lung Cancer</condition>
  <arm_group>
    <arm_group_label>Radiologists</arm_group_label>
    <description>Radiologists who have certification by the American Board of Radiology</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Software</intervention_name>
    <description>This is an observer performance study. Radiologists will interpret chest radiographs without and then with the Riverain software, both SoftView (TM) OnGuard (TM) CADe Software with be tested</description>
    <arm_group_label>Radiologists</arm_group_label>
    <other_name>SoftView 2.0</other_name>
    <other_name>OnGuard 5.1</other_name>
  </intervention>
  <eligibility>
    <study_pop>
      <textblock>
        Radiologists in active clinical practice who are not subspecialists in chest radiology
      </textblock>
    </study_pop>
    <sampling_method>Non-Probability Sample</sampling_method>
    <criteria>
      <textblock>
        Inclusion Criteria:

          -  US American Board of Radiology Certified Radiologists in active clinical practice

        Exclusion Criteria:

          -  Specialists in pulmonary or chest or cardio-pulmonary radiology Prior membership on
             expert panels for this study who prepared cases Current or recent colleagues or
             trainees (within 10 years) of the Principal Investigator
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>N/A</minimum_age>
    <maximum_age>N/A</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Matthew T. Freedman, MD, MBA</last_name>
    <role>Principal Investigator</role>
    <affiliation>Georgetown University</affiliation>
  </overall_official>
  <overall_official>
    <last_name>Ben Lo, Ph.D.</last_name>
    <role>Study Director</role>
    <affiliation>Georgetown University</affiliation>
  </overall_official>
  <location>
    <facility>
      <name>ISIS Imaging Science Research Center, Georgetown University</name>
      <address>
        <city>Washington</city>
        <state>District of Columbia</state>
        <zip>20057</zip>
        <country>United States</country>
      </address>
    </facility>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>August 2014</verification_date>
  <!-- For several months we've had both old and new date name tags                             -->
  <!-- Now, the old date names have been dropped.                                               -->
  <!-- The new date name replacements are:                                                      -->
  <!--     OLD (gone)                                        NEW (in use)                       -->
  <!--   lastchanged_date                         becomes   last_update_submitted               -->
  <!--   firstreceived_date                       becomes   study_first_submitted               -->
  <!--   firstreceived_results_date               becomes   results_first_submitted             -->
  <!--   firstreceived_results_disposition_date   becomes   disposition_first_submitted         -->
  <study_first_submitted>May 5, 2008</study_first_submitted>
  <study_first_submitted_qc>May 19, 2009</study_first_submitted_qc>
  <study_first_posted type="Estimate">May 21, 2009</study_first_posted>
  <results_first_submitted>September 19, 2013</results_first_submitted>
  <results_first_submitted_qc>August 20, 2014</results_first_submitted_qc>
  <results_first_posted type="Estimate">September 1, 2014</results_first_posted>
  <last_update_submitted>August 20, 2014</last_update_submitted>
  <last_update_submitted_qc>August 20, 2014</last_update_submitted_qc>
  <last_update_posted type="Estimate">September 1, 2014</last_update_posted>
  <responsible_party>
    <responsible_party_type>Principal Investigator</responsible_party_type>
    <investigator_affiliation>Georgetown University</investigator_affiliation>
    <investigator_full_name>Matthew T. Freedman, MD</investigator_full_name>
    <investigator_title>Associate Professor Oncology, Adjunct (Pending)</investigator_title>
  </responsible_party>
  <keyword>Lung Cancer</keyword>
  <keyword>Chest Radiograph</keyword>
  <keyword>Computer-aided Detection</keyword>
  <keyword>Image processing</keyword>

  <clinical_results>

    <participant_flow>
      <recruitment_details>Recruitment between April 9, 2009 and June 15, 2009. Recruited by email and phone calls to individuals meeting entry criteria specified in the protocl.</recruitment_details>
      <pre_assignment_details>All recruited individuals met entry criteria. None were excluded or dropped from the study. Training in the use of studied device (software) occurred immediately prior to the experiment. Training took 55 to 80 minutes, depending on the individuals speed. Each individual served as his/her own control, so all participants were in both groups</pre_assignment_details>
      <group_list>
        <group group_id="P1">
          <title>Radiologists</title>
          <description>Radiologists who have certification by the American Board of Radiology
Riverain OnGuard and SoftView Software : This is an observer performance study. Radiologists will interpret chest radiographs without and then with the Riverain software, Two types of software are tested: SoftView (TM). SoftView decreases the visibility of the ribs and clavicles on chest radiographs. OnGuard marks locations on chest radiographs meeting some of the software signs of lung nodules, a method often called Computer Aided Detection (CADe).</description>
        </group>
      </group_list>
      <period_list>
        <period>
          <title>Overall Study</title>
          <milestone_list>
            <milestone>
              <title>STARTED</title>
              <participants_list>
                <participants group_id="P1" count="15">Radiologists served as own control. Total enrollment is 15 radiologists.</participants>
              </participants_list>
            </milestone>
            <milestone>
              <title>COMPLETED</title>
              <participants_list>
                <participants group_id="P1" count="15"/>
              </participants_list>
            </milestone>
            <milestone>
              <title>NOT COMPLETED</title>
              <participants_list>
                <participants group_id="P1" count="0"/>
              </participants_list>
            </milestone>
          </milestone_list>
        </period>
      </period_list>
    </participant_flow>

    <baseline>
      <population>Power calculation was performed to determine the number of participants</population>
      <group_list>
        <group group_id="B1">
          <title>Radiologists</title>
          <description>Radiologists who have certification by the American Board of Radiology
Riverain OnGuard CAD Software : This is an observer performance study. Radiologists will interpret chest radiographs without and then with the Riverain software, both SoftView (TM) OnGuard (TM) CADe Software with be tested</description>
        </group>
      </group_list>
      <analyzed_list>
        <analyzed>
          <units>Participants</units>
          <scope>Overall</scope>
          <count_list>
            <count group_id="B1" value="15"/>
          </count_list>
        </analyzed>
      </analyzed_list>
      <measure_list>
        <measure>
          <title>Age, Customized</title>
          <description>&gt;= 25 years old</description>
          <units>participants</units>
          <param>Number</param>
          <class_list>
            <class>
              <title>number of participants</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="B1" value="15"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
        <measure>
          <title>Gender</title>
          <units>participants</units>
          <param>Number</param>
          <class_list>
            <class>
              <title>Female</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="B1" value="5"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
            <class>
              <title>Male</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="B1" value="10"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
        <measure>
          <title>Region of Enrollment</title>
          <units>participants</units>
          <param>Number</param>
          <class_list>
            <class>
              <title>United States</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="B1" value="15"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
      </measure_list>
    </baseline>

    <outcome_list>
      <outcome>
        <type>Primary</type>
        <title>Improvement in Cancer Detection as Measured by Localized Receiver Operating Characteristic) LROC Changes Under the LROC Curve.</title>
        <description>Standard methods for LROC methodology and statistical analysis were used. We are testing two different types of software using different cases, but the same radiologists to control for radiologist differences. LROC is Localized Receiver Operating Characteristic. LROC measures the trade-offs between sensitivity and specificity as radiologists use different levels of suspicion of disease. This analysis is for the software that decreases the visibility of the ribs and clavicles while preserving (and potentially enhancing) the visibility of the lungs and lung diseases. In this case, the level of suspicion recorded was for the radiologist's concern that a finding did or did not represent cancer. Please note that the FDA approved indications for use is to detected nodules that may represent cancer, but in our study scoring for a true finding was based on whether or not the nodule did represent cancer.
A larger number, if statistically significant, indicates that that method is better.</description>
        <time_frame>Three days of experiment over 3-5 months, varied by participant</time_frame>
        <population>122 subjects had cancer that potentially could be detected on their chest radiograph. Power analysis showed that sample size of 351 patients, in a 2:1 ratio of nodule absent to present patients was selected to provide 80% power to detect a difference in areas under the curve of 0.10 or greater.</population>
        <group_list>
          <group group_id="O1">
            <title>Radiologists Control for SoftView</title>
            <description>Control image interpretation unaided by software of either type. This is the control for the SoftView experiment. It uses the same radiologists (to avoid a bias that might result from using different radiologists) as the OnGuard Computer-aided detection software, but different cases.</description>
          </group>
          <group group_id="O2">
            <title>Radiologists Using Softview Software</title>
            <description>This software suppresses the visibility of the ribs and clavicles potentially revealing non-calcified nodules make less conspicuous by the bones projected on top of the nodules</description>
          </group>
        </group_list>
        <measure>
          <title>Improvement in Cancer Detection as Measured by Localized Receiver Operating Characteristic) LROC Changes Under the LROC Curve.</title>
          <description>Standard methods for LROC methodology and statistical analysis were used. We are testing two different types of software using different cases, but the same radiologists to control for radiologist differences. LROC is Localized Receiver Operating Characteristic. LROC measures the trade-offs between sensitivity and specificity as radiologists use different levels of suspicion of disease. This analysis is for the software that decreases the visibility of the ribs and clavicles while preserving (and potentially enhancing) the visibility of the lungs and lung diseases. In this case, the level of suspicion recorded was for the radiologist's concern that a finding did or did not represent cancer. Please note that the FDA approved indications for use is to detected nodules that may represent cancer, but in our study scoring for a true finding was based on whether or not the nodule did represent cancer.
A larger number, if statistically significant, indicates that that method is better.</description>
          <population>122 subjects had cancer that potentially could be detected on their chest radiograph. Power analysis showed that sample size of 351 patients, in a 2:1 ratio of nodule absent to present patients was selected to provide 80% power to detect a difference in areas under the curve of 0.10 or greater.</population>
          <units>unitless</units>
          <param>Mean</param>
          <dispersion>95% Confidence Interval</dispersion>
          <units_analyzed>Participants</units_analyzed>
          <analyzed_list>
            <analyzed>
              <units>Participants</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="15"/>
                <count group_id="O2" value="15"/>
              </count_list>
            </analyzed>
            <analyzed>
              <units>chest radiographs</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="368"/>
                <count group_id="O2" value="368"/>
              </count_list>
            </analyzed>
          </analyzed_list>
          <class_list>
            <class>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="0.460" lower_limit="0.435" upper_limit="0.486"/>
                    <measurement group_id="O2" value="0.558" lower_limit="0.539" upper_limit="0.578"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
        <analysis_list>
          <analysis>
            <group_id_list>
              <group_id>O1</group_id>
              <group_id>O2</group_id>
            </group_id_list>
            <groups_desc>The sample size of 351 patients, in a 2:1 ratio of nodule absent to present patients was selected to provide 80% power to detect a difference in areas under the curve of 0.10 or greater. This sample size was calculated using PASS software (Hintze, 2008). Settings: • α = 0.025. • one-sided test • AUCA = 0.80 and AUCSV = 0.90.• 2:1 ratio of patients with nodules to those without • areas calculated for the entire curves • correlation 0.3* • discrete data. • standard deviation ratios of 1*.</groups_desc>
            <non_inferiority_type>Non-Inferiority or Equivalence</non_inferiority_type>
            <non_inferiority_desc>Given a continuous confidence score (0-100), the trapezoidal method was used to obtain the area under the LROC with bootstrapping at 10,000 iterations to obtain the 95% confidence interval (CI). Acceptance criteria for tests of non-inferiority and superiority were previously set at 95% CI upper bound of ΔAUCUA-SV less than δ=0.1 and δ=0, respectively. If the 95% CI upper bound difference of UA-SV is less than 0.1, then SV is non-inferior. If it is 0 or less than 0, then SV is superior.</non_inferiority_desc>
            <p_value>0.05</p_value>
            <method>Bootstraping</method>
          </analysis>
        </analysis_list>
      </outcome>
      <outcome>
        <type>Other Pre-specified</type>
        <title>Difference in the Area Under the LROC Curve Comparing OnGuard 1.0 and OnGuard 5.1</title>
        <description>This reports the comparison of the detection of lung nodules that were proven to represent lung cancers. It compares the results of two versions of computer-aided detection software: OnGuard 1.0 from 2001 and OnGuard 5.1 from 2009. The results represent the responses of radiologists when they use one or the other types of software. To compare radiologists' results with the two types of software, the measurement analyzed was the difference in the areas under the localized receiver operating characteristic curve (LROC). The results from the 15 participating radiologists were averaged (mean value). The area under the LROC curve is a measure of the trade-offs between sensitivity and 1-specificity that occurs as the level of certainty of a positive finding changes. It is normally reported as a decimal without units. In this study dsign, a lower number indicates that the new method (OnGuard 5.1), if statistically significant, if better.</description>
        <time_frame>5 months</time_frame>
        <population>81 of the 263 radiographs contained a non-calcified nodule that had been diagnosed as lung cancer. Power calculation showed 246 patients, in a 2:1 ratio of nodule absent to present would provide 80% power to detect a difference in areas under the curve of 0.10 or greater.</population>
        <group_list>
          <group group_id="O1">
            <title>Radiologists Using OnGuard Software: Difference of 1.0 and 5.1</title>
            <description>Radiologists using OnGuard software. Two different versions were tested. OnGuard 1.0 and OnGuard 5.1. This software uses a computer algorithm to identify non-calcified lung nodules consistent with lung cancer. The value presented is the average difference in the areas under the LROC curve as demonstrated for the participating radiologists. The value for OnGuard 5.1 was subtracted from that of OnGuard 1.0, so a negative value indicates that OnGuard 5.1 had a higher value than OnGuard 1.0. The 95% confidence interval indicates that the OnGuard 5.1 was significantly improved.</description>
          </group>
        </group_list>
        <measure>
          <title>Difference in the Area Under the LROC Curve Comparing OnGuard 1.0 and OnGuard 5.1</title>
          <description>This reports the comparison of the detection of lung nodules that were proven to represent lung cancers. It compares the results of two versions of computer-aided detection software: OnGuard 1.0 from 2001 and OnGuard 5.1 from 2009. The results represent the responses of radiologists when they use one or the other types of software. To compare radiologists' results with the two types of software, the measurement analyzed was the difference in the areas under the localized receiver operating characteristic curve (LROC). The results from the 15 participating radiologists were averaged (mean value). The area under the LROC curve is a measure of the trade-offs between sensitivity and 1-specificity that occurs as the level of certainty of a positive finding changes. It is normally reported as a decimal without units. In this study dsign, a lower number indicates that the new method (OnGuard 5.1), if statistically significant, if better.</description>
          <population>81 of the 263 radiographs contained a non-calcified nodule that had been diagnosed as lung cancer. Power calculation showed 246 patients, in a 2:1 ratio of nodule absent to present would provide 80% power to detect a difference in areas under the curve of 0.10 or greater.</population>
          <units>unitless</units>
          <param>Mean</param>
          <dispersion>95% Confidence Interval</dispersion>
          <units_analyzed>Participants</units_analyzed>
          <analyzed_list>
            <analyzed>
              <units>Participants</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="15"/>
              </count_list>
            </analyzed>
            <analyzed>
              <units>number of radiographs</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="253"/>
              </count_list>
            </analyzed>
          </analyzed_list>
          <class_list>
            <class>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="-0.043" lower_limit="-0.075" upper_limit="-0.010"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
      </outcome>
      <outcome>
        <type>Secondary</type>
        <title>Sensitivity and Specificity Using SoftView Software</title>
        <description>Sensitivity and specificity were calculated using the radiologists' responses of recommendations for follow-up with CT or biopsy. Truth was whether or not the nodule identified was found to be cancer. Sensitivity is the percentage of correct identification of a positive case (a case with cancer). Specificity is the percentage of negative cases (those without cancer) that were correctly identified as not having cancer. The mean values of 15 radiologists are used.</description>
        <time_frame>Three days of experiment over 3-5 months, varied by participant</time_frame>
        <group_list>
          <group group_id="O1">
            <title>Radiologists Control for SoftView</title>
            <description>Control image interpretation unaided by software of either type. This is the control for the SoftView experiment. It uses the same radiologists (to avoid a bias that might result from using different radiologists) as the OnGuard Computer-aided detection software, but different cases.</description>
          </group>
          <group group_id="O2">
            <title>Radiologists Using Softview Software</title>
            <description>This software suppresses the visibility of the ribs and clavicles potentially revealing non-calcified nodules make less conspicuous by the bones projected on top of the nodules</description>
          </group>
        </group_list>
        <measure>
          <title>Sensitivity and Specificity Using SoftView Software</title>
          <description>Sensitivity and specificity were calculated using the radiologists' responses of recommendations for follow-up with CT or biopsy. Truth was whether or not the nodule identified was found to be cancer. Sensitivity is the percentage of correct identification of a positive case (a case with cancer). Specificity is the percentage of negative cases (those without cancer) that were correctly identified as not having cancer. The mean values of 15 radiologists are used.</description>
          <units>percentage of cases</units>
          <param>Mean</param>
          <dispersion>95% Confidence Interval</dispersion>
          <analyzed_list>
            <analyzed>
              <units>Participants</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="15"/>
                <count group_id="O2" value="15"/>
              </count_list>
            </analyzed>
          </analyzed_list>
          <class_list>
            <class>
              <title>Sensitivity</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="49.5" lower_limit="45.9" upper_limit="53.0"/>
                    <measurement group_id="O2" value="66.3" lower_limit="63.1" upper_limit="69.7"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
            <class>
              <title>Specificity</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="96.1" lower_limit="95.0" upper_limit="97.1"/>
                    <measurement group_id="O2" value="91.8" lower_limit="89.9" upper_limit="93.5"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
      </outcome>
    </outcome_list>

    <reported_events>
      <time_frame>5 months</time_frame>
      <group_list>
        <group group_id="E1">
          <title>Participants</title>
          <description>The 15 radiologists who participated</description>
        </group>
      </group_list>
      <serious_events>
        <category_list>
          <category>
            <title>Total</title>
            <event_list>
              <event>
                <sub_title>Total, serious adverse events</sub_title>
                <counts group_id="E1" subjects_affected="0" subjects_at_risk="15"/>
              </event>
            </event_list>
          </category>
        </category_list>
      </serious_events>
      <other_events>
        <frequency_threshold>5</frequency_threshold>
        <category_list>
          <category>
            <title>Total</title>
            <event_list>
              <event>
                <sub_title>Total, other adverse events</sub_title>
                <counts group_id="E1" subjects_affected="0" subjects_at_risk="15"/>
              </event>
            </event_list>
          </category>
        </category_list>
      </other_events>
    </reported_events>

    <certain_agreements>
      <pi_employee>Principal Investigators are NOT employed by the organization sponsoring the study.</pi_employee>
      <restrictive_agreement>Sponsor can review results communications prior to public release; review by sponsor must be completed within 60 days. Public release of results cannot occur prior to the completion of the FDA review of submitted results.</restrictive_agreement>
    </certain_agreements>
    <limitations_and_caveats>No adverse events occurred. Study was subject to the following potential biases: Sample bias; Majority rule bias; Referral Bias, bias from knowledge that this is an experiment; reader order bias; learning bias; Bias from use of new technology, et al</limitations_and_caveats>
    <point_of_contact>
      <name_or_title>Matthew T. Freedman</name_or_title>
      <organization>Georgetown University Medical Center</organization>
      <phone>4105429680</phone>
      <email>MTFreedman@verizon.net</email>
    </point_of_contact>
  </clinical_results>
</clinical_study>

