<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on May 29, 2018</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT03174314</url>
  </required_header>
  <id_info>
    <org_study_id>17-00317</org_study_id>
    <nct_id>NCT03174314</nct_id>
  </id_info>
  <brief_title>Feasibility and Efficacy of Assisstive Tactile and Auditory Communicating Devices</brief_title>
  <acronym>VIS4ION</acronym>
  <official_title>Feasibility and Efficacy of the VIS4ION Platform and Assistive Tactile and Auditory Communicating Devices in Low Vision Subjects</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>New York University School of Medicine</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
  </sponsors>
  <source>New York University School of Medicine</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      This pilot study will integrate multi-sensor fusion techniques (software) to effectively
      combine information obtained from the newly embedded infrared, ultrasound, and
      stereo-camera-based sensor systems (hardware) that are implemented into the VIS4ION platform.

      The core of this technology is based on 4 components: (1) a wearable vest with several
      distinct range and image sensors embedded. These sensors extract pertinent information about
      obstacles and the environment, which are conveyed to (2) a haptic interface (belt) that
      communicates this spatial information to the end-user in real-time via an intuitive,
      ergonomic and personalized vibrotactile re-display along the torso. (3) A smartphone serves
      as a connectivity gateway and coordinates the core components through WiFi, bluetooth, and/or
      4G LTE, (4) a headset that contains both binaural, open-ear, bone conduction speakers
      (leaving the ear canal patent for ambient sounds) and a microphone for oral
      communication-based voice recognition during use of a virtual personal assistant (VPA).

      Blindfolded-sighted (50), and blind (50) subjects in a real-world, combined obstacle
      avoidance / navigation task will serve as an independent measure of overall improvements in
      the system as well as a roadmap for future avenues to enhance performance.
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Investigators will try to develop algorithms that will recognize multiple objects and persons
      in real-time (enhanced scene interpretation for multi-object identification). And based on
      that, human-centered simulation trials and experiments for feasibility and efficacy of the
      platform's tactile and auditory 'communication' outputs will be conducted. Finally auditory
      and tactile 'prompts' (system output) based on the end user's immediate needs based on
      initial testing results, will be integrated into the platform.
    </textblock>
  </detailed_description>
  <overall_status>Not yet recruiting</overall_status>
  <start_date type="Anticipated">June 2018</start_date>
  <completion_date type="Anticipated">December 2019</completion_date>
  <primary_completion_date type="Anticipated">December 2018</primary_completion_date>
  <phase>N/A</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>Non-Randomized</allocation>
    <intervention_model>Parallel Assignment</intervention_model>
    <primary_purpose>Treatment</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Real-time situational obstacle awareness measured by detection threshold</measure>
    <time_frame>6 Months</time_frame>
    <description>Ability of observers to, in real-time, interpret and respond to platform outputs will be tested</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Real-time situational obstacle awareness measured by change in time</measure>
    <time_frame>6 Months</time_frame>
    <description>Ability of observers to, in real-time, interpret and respond to platform outputs will be tested; ∆t averages (separately by in-/correct)</description>
  </primary_outcome>
  <number_of_arms>2</number_of_arms>
  <enrollment type="Anticipated">100</enrollment>
  <condition>Visual Impairment</condition>
  <arm_group>
    <arm_group_label>50 visually impaired</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>50 visually impaired subjects will provide continuity across iterations of the testing, allowing for direct comparisons of responses within an individual for a single behavioral measure across different iterations of the device architecture and output configuration.</description>
  </arm_group>
  <arm_group>
    <arm_group_label>50 healthy controls</arm_group_label>
    <arm_group_type>Active Comparator</arm_group_type>
    <description>50 healthy (naive) subjects invaluable insights as well as a range of body types, cognitive abilities, and other idiosyncrasies that will keep our design and testing process from tailoring the device to a small set of individuals rather than the broader population.</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>standard object battery and training sequence</intervention_name>
    <description>Used to examine human localization and identification performance. Object battery will include: trip hazards ('pucks' with various height/width combinations to represent children's toys, street debris, rocks, pets, etc.), furniture (chairs, desks, couches, benches), people, walls, and corridors. Objects will be recorded by placing them to the right or left on the path-of-travel. Raw sensor outputs for single obstacles and walls in isolation (trip-hazard pucks with 8 heights, chair, desk, person, wall) and the same set of single obstacles against a wall. This set of 23 raw sensor traces can be used in various combinations (e.g., encountering a curb and then a person, or a chair) to create a training sequence that will help experimentally naïve subjects to understand the correspondence between tactile stimulation and the real-world scenario it is intended to depict.</description>
    <arm_group_label>50 visually impaired</arm_group_label>
    <arm_group_label>50 healthy controls</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:

          -  People with visual impairments of all different levels and etiologies.

        Exclusion Criteria:

          -  Significant cognitive dysfunction (score &lt;24 on Folsteins' Mini Mental Status
             Examination)

          -  Previous neurological illness, complicated medical condition;

          -  Significant mobility restrictions; people using walkers and wheelchairs

          -  Pregnancy
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>18 Years</minimum_age>
    <maximum_age>80 Years</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>John R Rizzo, MD</last_name>
    <role>Principal Investigator</role>
    <affiliation>New York University School of Medicine</affiliation>
  </overall_official>
  <overall_contact>
    <last_name>Zena Moore</last_name>
    <phone>646 501 7828</phone>
    <email>zena.moore@nyumc.org</email>
  </overall_contact>
  <overall_contact_backup>
    <last_name>James Fung</last_name>
    <email>james.fung@nyumc.org</email>
  </overall_contact_backup>
  <location>
    <facility>
      <name>New York University School of Medicine</name>
      <address>
        <city>New York</city>
        <state>New York</state>
        <zip>10016</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Not yet recruiting</status>
    <contact>
      <last_name>Zena Moore</last_name>
      <phone>646-501-7828</phone>
      <email>zena.moore@nyumc.org</email>
    </contact>
    <investigator>
      <last_name>John R Rizzo, MD</last_name>
      <role>Principal Investigator</role>
    </investigator>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>February 2018</verification_date>
  <!-- For several months we've had both old and new date name tags                             -->
  <!-- Now, the old date names have been dropped.                                               -->
  <!-- The new date name replacements are:                                                      -->
  <!--     OLD (gone)                                        NEW (in use)                       -->
  <!--   lastchanged_date                         becomes   last_update_submitted               -->
  <!--   firstreceived_date                       becomes   study_first_submitted               -->
  <!--   firstreceived_results_date               becomes   results_first_submitted             -->
  <!--   firstreceived_results_disposition_date   becomes   disposition_first_submitted         -->
  <study_first_submitted>May 30, 2017</study_first_submitted>
  <study_first_submitted_qc>May 30, 2017</study_first_submitted_qc>
  <study_first_posted type="Actual">June 2, 2017</study_first_posted>
  <last_update_submitted>February 1, 2018</last_update_submitted>
  <last_update_submitted_qc>February 1, 2018</last_update_submitted_qc>
  <last_update_posted type="Actual">February 5, 2018</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>Visual Impairment</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Vision Disorders</mesh_term>
    <mesh_term>Vision, Low</mesh_term>
  </condition_browse>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>

