<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on May 29, 2018</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT01821534</url>
  </required_header>
  <id_info>
    <org_study_id>191622-128</org_study_id>
    <nct_id>NCT01821534</nct_id>
  </id_info>
  <brief_title>Reliability of a Masseter Muscle Prominence Scale and Lower Facial Shape Classification</brief_title>
  <sponsors>
    <lead_sponsor>
      <agency>Allergan</agency>
      <agency_class>Industry</agency_class>
    </lead_sponsor>
  </sponsors>
  <source>Allergan</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
  </oversight_info>
  <brief_summary>
    <textblock>
      This study will evaluate the inter-rater and intra-rater reliability of a Masseter Muscle
      Prominence Scale for evaluating a patient's muscle prominence and a Lower Shape
      Classification for evaluating a patient's lower facial shape.
    </textblock>
  </brief_summary>
  <overall_status>Completed</overall_status>
  <start_date>March 2013</start_date>
  <completion_date type="Actual">May 2013</completion_date>
  <primary_completion_date type="Actual">May 2013</primary_completion_date>
  <study_type>Observational</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <observational_model>Cohort</observational_model>
    <time_perspective>Prospective</time_perspective>
  </study_design_info>
  <primary_outcome>
    <measure>Inter-rater Reliability Using a Masseter Muscle Prominence Scale (MMPS)</measure>
    <time_frame>Day 1</time_frame>
    <description>The MMPS is an ordinal tool to assess the masseter muscle prominence (jaw muscle) for each side of the face from 1=minimal to 5=very marked. Inter-rater (among raters) reliability was calculated separately for the left and right side of the face using Kendall's coefficient of concordance (Kendall's W). Kendall W statistics overall for the left and right sides of the face were derived using the average of assessment 1 and assessment 2 rounded to the nearest whole integer for each subject and each clinician. A total of 8 physicians rated each subject. The degree of agreement of the point estimates of Kendall's W was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kendall's W is provided.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Intra-rater Reliability Using a MMPS</measure>
    <time_frame>Day 1</time_frame>
    <description>The MMPS is an ordinal tool to assess the masseter muscle prominence (jaw muscle) for each side of the face from 1 = minimal to 5 = very marked. Intra-rater (within raters) reliability was calculated separately for the left and right side of the face using weighted Kappa statistics. Weighted Kappa statistics were calculated for each of the 8 physician raters. The overall intra-rater agreement for Kappa statistics for all raters combined was estimated by pooling Kappa statistics for each rater using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Inter-rater Reliability Using a Lower Facial Shape Classification (LFSC)</measure>
    <time_frame>Day 1</time_frame>
    <description>The LFSC is a qualitative tool to assess facial shape into one of 5 categories (A, B, C, D, and E). Inter-rater (among raters) reliability was calculated using Kappa statistics. Kappa statistics were calculated for each of the 5 facial categories. A total of 8 physicians rated each subject. The overall inter-rater agreement for Kappa statistics for all categories combined was estimated by pooling Kappa statistics for each category using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Intra-rater Reliability Using a LFSC</measure>
    <time_frame>Day 1</time_frame>
    <description>The LFSC is a qualitative tool to assess facial shape into one of 5 categories (A, B, C, D, and E). Intra-rater (within raters) reliability was calculated using Kappa statistics. Kappa statistics were calculated for each of the 8 physician raters. The overall intra-rater agreement for Kappa statistics for all raters combined was estimated by pooling Kappa statistics for each rater using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
  </primary_outcome>
  <number_of_groups>1</number_of_groups>
  <enrollment type="Actual">201</enrollment>
  <condition>Healthy Volunteers</condition>
  <arm_group>
    <arm_group_label>All Participants</arm_group_label>
    <description>Healthy volunteers. No treatment (intervention) was administered.</description>
  </arm_group>
  <intervention>
    <intervention_type>Other</intervention_type>
    <intervention_name>No Intervention</intervention_name>
    <description>No treatment (intervention) was administered.</description>
    <arm_group_label>All Participants</arm_group_label>
  </intervention>
  <eligibility>
    <study_pop>
      <textblock>
        Healthy Volunteers
      </textblock>
    </study_pop>
    <sampling_method>Probability Sample</sampling_method>
    <criteria>
      <textblock>
        Inclusion Criteria:

        -sufficient visual acuity without the use of glasses or with contact lenses to self-assess
        lower facial shape in a mirror

        Exclusion Criteria:

          -  infection of the mouth or gums, or facial skin infection requiring antibiotics

          -  planned dental or facial procedure

          -  unwillingness to be photographed and have the photos used for research, training, or
             educational purposes
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>18 Years</minimum_age>
    <maximum_age>N/A</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Medical Director</last_name>
    <role>Study Director</role>
    <affiliation>Allergan</affiliation>
  </overall_official>
  <location>
    <facility>
      <address>
        <city>Newport Beach</city>
        <state>California</state>
        <country>United States</country>
      </address>
    </facility>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>March 2014</verification_date>
  <!-- For several months we've had both old and new date name tags                             -->
  <!-- Now, the old date names have been dropped.                                               -->
  <!-- The new date name replacements are:                                                      -->
  <!--     OLD (gone)                                        NEW (in use)                       -->
  <!--   lastchanged_date                         becomes   last_update_submitted               -->
  <!--   firstreceived_date                       becomes   study_first_submitted               -->
  <!--   firstreceived_results_date               becomes   results_first_submitted             -->
  <!--   firstreceived_results_disposition_date   becomes   disposition_first_submitted         -->
  <study_first_submitted>March 25, 2013</study_first_submitted>
  <study_first_submitted_qc>March 25, 2013</study_first_submitted_qc>
  <study_first_posted type="Estimate">April 1, 2013</study_first_posted>
  <results_first_submitted>March 28, 2014</results_first_submitted>
  <results_first_submitted_qc>March 28, 2014</results_first_submitted_qc>
  <results_first_posted type="Estimate">April 30, 2014</results_first_posted>
  <last_update_submitted>March 28, 2014</last_update_submitted>
  <last_update_submitted_qc>March 28, 2014</last_update_submitted_qc>
  <last_update_posted type="Estimate">April 30, 2014</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>

  <clinical_results>

    <participant_flow>
      <group_list>
        <group group_id="P1">
          <title>All Participants</title>
          <description>Healthy volunteers. No treatment (intervention) was administered.</description>
        </group>
      </group_list>
      <period_list>
        <period>
          <title>Overall Study</title>
          <milestone_list>
            <milestone>
              <title>STARTED</title>
              <participants_list>
                <participants group_id="P1" count="201"/>
              </participants_list>
            </milestone>
            <milestone>
              <title>COMPLETED</title>
              <participants_list>
                <participants group_id="P1" count="201"/>
              </participants_list>
            </milestone>
            <milestone>
              <title>NOT COMPLETED</title>
              <participants_list>
                <participants group_id="P1" count="0"/>
              </participants_list>
            </milestone>
          </milestone_list>
        </period>
      </period_list>
    </participant_flow>

    <baseline>
      <group_list>
        <group group_id="B1">
          <title>All Participants</title>
          <description>Healthy volunteers. No treatment (intervention) was administered.</description>
        </group>
      </group_list>
      <analyzed_list>
        <analyzed>
          <units>Participants</units>
          <scope>Overall</scope>
          <count_list>
            <count group_id="B1" value="201"/>
          </count_list>
        </analyzed>
      </analyzed_list>
      <measure_list>
        <measure>
          <title>Age</title>
          <units>Years</units>
          <param>Mean</param>
          <dispersion>Standard Deviation</dispersion>
          <class_list>
            <class>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="B1" value="35.5" spread="11.43"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
        <measure>
          <title>Gender</title>
          <units>Participants</units>
          <param>Number</param>
          <class_list>
            <class>
              <title>Female</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="B1" value="128"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
            <class>
              <title>Male</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="B1" value="73"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
      </measure_list>
    </baseline>

    <outcome_list>
      <outcome>
        <type>Primary</type>
        <title>Inter-rater Reliability Using a Masseter Muscle Prominence Scale (MMPS)</title>
        <description>The MMPS is an ordinal tool to assess the masseter muscle prominence (jaw muscle) for each side of the face from 1=minimal to 5=very marked. Inter-rater (among raters) reliability was calculated separately for the left and right side of the face using Kendall’s coefficient of concordance (Kendall's W). Kendall W statistics overall for the left and right sides of the face were derived using the average of assessment 1 and assessment 2 rounded to the nearest whole integer for each subject and each clinician. A total of 8 physicians rated each subject. The degree of agreement of the point estimates of Kendall's W was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kendall's W is provided.</description>
        <time_frame>Day 1</time_frame>
        <population>Reliability population: all subjects with at least assessment 1 performed by at least 1 in-person rater on day 1</population>
        <group_list>
          <group group_id="O1">
            <title>All Participants</title>
            <description>Healthy volunteers. No treatment (intervention) was administered.</description>
          </group>
        </group_list>
        <measure>
          <title>Inter-rater Reliability Using a Masseter Muscle Prominence Scale (MMPS)</title>
          <description>The MMPS is an ordinal tool to assess the masseter muscle prominence (jaw muscle) for each side of the face from 1=minimal to 5=very marked. Inter-rater (among raters) reliability was calculated separately for the left and right side of the face using Kendall’s coefficient of concordance (Kendall's W). Kendall W statistics overall for the left and right sides of the face were derived using the average of assessment 1 and assessment 2 rounded to the nearest whole integer for each subject and each clinician. A total of 8 physicians rated each subject. The degree of agreement of the point estimates of Kendall's W was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kendall's W is provided.</description>
          <population>Reliability population: all subjects with at least assessment 1 performed by at least 1 in-person rater on day 1</population>
          <units>Kendall's W</units>
          <param>Number</param>
          <dispersion>95% Confidence Interval</dispersion>
          <analyzed_list>
            <analyzed>
              <units>Participants</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="201"/>
              </count_list>
            </analyzed>
          </analyzed_list>
          <class_list>
            <class>
              <title>Left Side</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="0.728" lower_limit="0.677" upper_limit="0.786"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
            <class>
              <title>Right Side</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="0.711" lower_limit="0.661" upper_limit="0.767"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
      </outcome>
      <outcome>
        <type>Primary</type>
        <title>Intra-rater Reliability Using a MMPS</title>
        <description>The MMPS is an ordinal tool to assess the masseter muscle prominence (jaw muscle) for each side of the face from 1 = minimal to 5 = very marked. Intra-rater (within raters) reliability was calculated separately for the left and right side of the face using weighted Kappa statistics. Weighted Kappa statistics were calculated for each of the 8 physician raters. The overall intra-rater agreement for Kappa statistics for all raters combined was estimated by pooling Kappa statistics for each rater using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
        <time_frame>Day 1</time_frame>
        <population>Reliability population: all subjects with at least assessment 1 performed by at least 1 in-person rater on day 1</population>
        <group_list>
          <group group_id="O1">
            <title>All Participants</title>
            <description>Healthy volunteers. No treatment (intervention) was administered.</description>
          </group>
        </group_list>
        <measure>
          <title>Intra-rater Reliability Using a MMPS</title>
          <description>The MMPS is an ordinal tool to assess the masseter muscle prominence (jaw muscle) for each side of the face from 1 = minimal to 5 = very marked. Intra-rater (within raters) reliability was calculated separately for the left and right side of the face using weighted Kappa statistics. Weighted Kappa statistics were calculated for each of the 8 physician raters. The overall intra-rater agreement for Kappa statistics for all raters combined was estimated by pooling Kappa statistics for each rater using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
          <population>Reliability population: all subjects with at least assessment 1 performed by at least 1 in-person rater on day 1</population>
          <units>Kappa statistics</units>
          <param>Number</param>
          <dispersion>95% Confidence Interval</dispersion>
          <analyzed_list>
            <analyzed>
              <units>Participants</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="201"/>
              </count_list>
            </analyzed>
          </analyzed_list>
          <class_list>
            <class>
              <title>Left Side</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="0.634" lower_limit="0.607" upper_limit="0.660"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
            <class>
              <title>Right Side</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="0.638" lower_limit="0.611" upper_limit="0.665"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
      </outcome>
      <outcome>
        <type>Primary</type>
        <title>Inter-rater Reliability Using a Lower Facial Shape Classification (LFSC)</title>
        <description>The LFSC is a qualitative tool to assess facial shape into one of 5 categories (A, B, C, D, and E). Inter-rater (among raters) reliability was calculated using Kappa statistics. Kappa statistics were calculated for each of the 5 facial categories. A total of 8 physicians rated each subject. The overall inter-rater agreement for Kappa statistics for all categories combined was estimated by pooling Kappa statistics for each category using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
        <time_frame>Day 1</time_frame>
        <population>Reliability population: all subjects with at least assessment 1 performed by at least 1 in-person rater on day 1</population>
        <group_list>
          <group group_id="O1">
            <title>All Participants</title>
            <description>Healthy volunteers. No treatment (intervention) was administered.</description>
          </group>
        </group_list>
        <measure>
          <title>Inter-rater Reliability Using a Lower Facial Shape Classification (LFSC)</title>
          <description>The LFSC is a qualitative tool to assess facial shape into one of 5 categories (A, B, C, D, and E). Inter-rater (among raters) reliability was calculated using Kappa statistics. Kappa statistics were calculated for each of the 5 facial categories. A total of 8 physicians rated each subject. The overall inter-rater agreement for Kappa statistics for all categories combined was estimated by pooling Kappa statistics for each category using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
          <population>Reliability population: all subjects with at least assessment 1 performed by at least 1 in-person rater on day 1</population>
          <units>Kappa Statistics</units>
          <param>Number</param>
          <dispersion>95% Confidence Interval</dispersion>
          <analyzed_list>
            <analyzed>
              <units>Participants</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="201"/>
              </count_list>
            </analyzed>
          </analyzed_list>
          <class_list>
            <class>
              <title>Assessment 1</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="0.358" lower_limit="0.343" upper_limit="0.372"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
            <class>
              <title>Assessment 2</title>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="0.352" lower_limit="0.337" upper_limit="0.367"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
      </outcome>
      <outcome>
        <type>Primary</type>
        <title>Intra-rater Reliability Using a LFSC</title>
        <description>The LFSC is a qualitative tool to assess facial shape into one of 5 categories (A, B, C, D, and E). Intra-rater (within raters) reliability was calculated using Kappa statistics. Kappa statistics were calculated for each of the 8 physician raters. The overall intra-rater agreement for Kappa statistics for all raters combined was estimated by pooling Kappa statistics for each rater using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
        <time_frame>Day 1</time_frame>
        <population>Reliability population: all subjects with at least assessment 1 performed by at least 1 in-person rater on day 1</population>
        <group_list>
          <group group_id="O1">
            <title>All Participants</title>
            <description>Healthy volunteers. No treatment (intervention) was administered.</description>
          </group>
        </group_list>
        <measure>
          <title>Intra-rater Reliability Using a LFSC</title>
          <description>The LFSC is a qualitative tool to assess facial shape into one of 5 categories (A, B, C, D, and E). Intra-rater (within raters) reliability was calculated using Kappa statistics. Kappa statistics were calculated for each of the 8 physician raters. The overall intra-rater agreement for Kappa statistics for all raters combined was estimated by pooling Kappa statistics for each rater using a chi-square statistic. The degree of agreement of the point estimates of Kappa statistics was interpreted according to the reference range scale that was pre-defined as: ≤0: poor, &gt;0 to ≤0.2: slight, &gt;0.2 to ≤0.4: fair, &gt;0.4 to ≤0.6: moderate, &gt;0.6 to ≤0.8: substantial, and &gt;0.8 to ≤1.0: almost perfect. The 95% confidence interval for Kappa statistics is provided.</description>
          <population>Reliability population: all subjects with at least assessment 1 performed by at least 1 in-person rater on day 1</population>
          <units>Kappa statistics</units>
          <param>Number</param>
          <dispersion>95% Confidence Interval</dispersion>
          <analyzed_list>
            <analyzed>
              <units>Participants</units>
              <scope>Measure</scope>
              <count_list>
                <count group_id="O1" value="201"/>
              </count_list>
            </analyzed>
          </analyzed_list>
          <class_list>
            <class>
              <category_list>
                <category>
                  <measurement_list>
                    <measurement group_id="O1" value="0.658" lower_limit="0.629" upper_limit="0.686"/>
                  </measurement_list>
                </category>
              </category_list>
            </class>
          </class_list>
        </measure>
      </outcome>
    </outcome_list>

    <reported_events>
      <time_frame>Adverse events (AEs) were collected during the study period (Screening to Day 1).</time_frame>
      <group_list>
        <group group_id="E1">
          <title>All Participants</title>
          <description>Healthy volunteers. No treatment (intervention) was administered.</description>
        </group>
      </group_list>
      <serious_events>
        <category_list>
          <category>
            <title>Total</title>
            <event_list>
              <event>
                <sub_title>Total, serious adverse events</sub_title>
                <counts group_id="E1" subjects_affected="0" subjects_at_risk="201"/>
              </event>
            </event_list>
          </category>
        </category_list>
      </serious_events>
      <other_events>
        <frequency_threshold>5</frequency_threshold>
        <category_list>
          <category>
            <title>Total</title>
            <event_list>
              <event>
                <sub_title>Total, other adverse events</sub_title>
                <counts group_id="E1" subjects_affected="0" subjects_at_risk="201"/>
              </event>
            </event_list>
          </category>
        </category_list>
      </other_events>
    </reported_events>

    <certain_agreements>
      <pi_employee>Principal Investigators are NOT employed by the organization sponsoring the study.</pi_employee>
      <restrictive_agreement>A disclosure restriction on the PI is that the sponsor can review results communications prior to public release and can embargo communications regarding trial results for a period that is less than or equal to 90 days from the time submitted to the sponsor for review. The sponsor cannot require changes to the communication and cannot extend the embargo.</restrictive_agreement>
    </certain_agreements>
    <point_of_contact>
      <name_or_title>Therapeutic Area Head,</name_or_title>
      <organization>Allergan, Inc</organization>
      <phone>714-246-4500</phone>
      <email>clinicaltrials@allergan.com</email>
    </point_of_contact>
  </clinical_results>
</clinical_study>

